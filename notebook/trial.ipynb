{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline, GPTQConfig, pipeline\n",
    "from auto_gptq import exllama_set_max_input_length\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import spacy\n",
    "\n",
    "from math import ceil\n",
    "from spacy.lang.en import English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux detected.\n",
      "the current wd is  /home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp\n"
     ]
    }
   ],
   "source": [
    "if platform.system() != 'Linux':\n",
    "    \n",
    "    print(\"GPTQ is optimized for NV cards on cuda-enabled Linux. Please check your OS\")\n",
    "\n",
    "else :\n",
    "    \n",
    "    os.chdir(\"/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp\")\n",
    "    print(\"Linux detected.\")\n",
    "    print(\"the current wd is \", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ca77dda8e64e788e12d09af00f2a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"/home/ning/workdir/model/wd-v0.0.1\"\n",
    "# model_name_or_path = \"TheBloke/Llama-2-70B-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"gptq-4bit-128g-actorder_True\")\n",
    "\n",
    "# model = exllama_set_max_input_length(model, 2048)\n",
    "\n",
    "# model.save_pretrained(\"/home/ning/workdir/model/docsum-testmodel-v0.0.1\", from_pt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_wrd = \"Summary:\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-GPTQ\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sliding windows : evaluating prompts $\\geqslant$ 4096 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_transformer_GPTQ/data/feature/test1.txt\") as f:\n",
    "    doc = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "result = nlp(doc)\n",
    "\n",
    "window_size = 1000\n",
    "grid_size = 2000\n",
    "shards_number = ceil(len(result)/grid_size)\n",
    "file_shards = [None] * shards_number\n",
    "\n",
    "for i in range(shards_number):\n",
    "\n",
    "    tmp = result[i *grid_size : i * grid_size + window_size]\n",
    "    file_shards[i] = ''.join([token.text_with_ws for token in tmp])\n",
    "\n",
    "    with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_transformer_GPTQ/data/feature/test1_' + str(i) + '.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(file_shards[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    \n",
      "Hami Qutami, who tried to arrange a murder, was given a harsher sentence increase of around 29 months by the court because he committed a severe crime and needed punishment that matched the harm done to society. The judges thought about many things when deciding the length of his punishment, including his help with police officers, his past, and his current health. Even though Qutami asked for a lighter sentence of about five years, the court decided that this punishment wouldn't fit the crime he committed.\n",
      "\n",
      "The government argued that reducing the time before parole was wrong and wanted to keep the original long period on parole. However, Qutami believed his sentence was correct and balanced the seriousness of his actions and the special circumstances of his situation. He also stated that the court should not intervene without strong evidence, and that parts of his argument might be considered twice, causing unfairness.\n",
      "\n",
      "Finally, the court decided that while Qutami's initial sentence was too short, it wasn't so unfair that it needed changing. They noted that the judge considered everything carefully and gave each factor proper importance. Despite receiving a nice reduction for admitting guilt, the judge's method for dealing with the period until parole was controversial.\n"
     ]
    }
   ],
   "source": [
    "short_sum = []\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=1500,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "for i in range(shards_number):\n",
    "\n",
    "    with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_transformer_GPTQ/data/feature/test1_' + str(i) + '.txt', 'r+', encoding=\"utf-8\") as file:\n",
    "        context = file.read()\n",
    "\n",
    "\n",
    "\n",
    "    prompt_template=f'''You are a helpful and patient AI assistant. You always pay attentions to all the details. Take a deep breathe and write a summary step by step about the following artile. Include all the details. \\n\n",
    "    Article: {short_sum} \\n {context} \\n \n",
    "    Summary: \n",
    "    '''\n",
    "\n",
    "    result = pipe(prompt_template)\n",
    "\n",
    "    raw_short_result = result[0]['generated_text']\n",
    "    short_sum_idx = raw_short_result.find(start_wrd)\n",
    "    short_sum = raw_short_result[short_sum_idx+8:]\n",
    "\n",
    "with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_transformer_GPTQ/result/cumulated_sum.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(short_sum)\n",
    "\n",
    "print(short_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating prompts $\\leqslant$ 4096 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "This article presents a detailed medical report regarding a 55-year-old male patient named Mr. Tan Ah Kow. The report includes essential patient's particulars like his full name, NRIC number, and current residential address. It highlights the doctor's credentials and their professional relationship. \n",
      "\n",
      "The document provides comprehensive insights into the patient's medical history, including previous health issues such as hypertension, hyperlipidemia, multiple strokes, cardiomyopathy, cardiac failure, and chronic renal disease. These complications led to numerous hospitalizations, transfers between hospitals, and treatments.\n",
      "\n",
      "It further describes the patient's behavioral and psychological symptoms associated with Dementia, supported by findings from various physical and mental state examinations. These include observations of his mood, orientation to time, place, and person, responses to simple arithmetic tasks, understanding of financial matters, and knowledge of his medical condition.\n",
      "\n",
      "Based on these assessments, the report concludes that the patient suffers from two primary diagnoses - Dementia and Stroke. Furthermore, it emphasizes how these conditions affect his mental capacities, significantly impacting his ability to manage personal and financial affairs. Consequently, the doctor believes that Mr. Tan Ah Kow lacks the necessary competence to handle his property and financial matters independently.\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_transformer_GPTQ/data/feature/os-1.txt\") as f:\n",
    "    context = f.read()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=4096,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=30,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "short_prompt_template=f'''You are a helpful and patient AI assistant. You always pay attentions to all the details. Take a deep breathe and write a long summary step by step about the following artile. Include all the details. \\n\n",
    " Article: {context} \\n \n",
    " Summary: \n",
    "'''\n",
    "\n",
    "result = pipe(short_prompt_template)\n",
    "\n",
    "raw_short_result = result[0]['generated_text']\n",
    "short_sum_idx = raw_short_result.find(start_wrd)\n",
    "short_sum = raw_short_result[short_sum_idx+8:]\n",
    "print(short_sum)\n",
    "\n",
    "with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_transformer_GPTQ/result/short_sum.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(short_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
