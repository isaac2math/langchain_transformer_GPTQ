{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline, GPTQConfig, pipeline\n",
    "from auto_gptq import exllama_set_max_input_length\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import spacy\n",
    "\n",
    "from math import ceil\n",
    "from spacy.lang.en import English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux detected.\n",
      "the current wd is  /home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp\n"
     ]
    }
   ],
   "source": [
    "if platform.system() != 'Linux':\n",
    "    \n",
    "    print(\"GPTQ is optimized for NV cards on cuda-enabled Linux. Please check your OS\")\n",
    "\n",
    "else :\n",
    "    \n",
    "    os.chdir(\"/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp\")\n",
    "    print(\"Linux detected.\")\n",
    "    print(\"the current wd is \", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9137187bbe4f788cae1b583fb979c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"/home/ning/workdir/model/wd-v0.0.1\"\n",
    "# model_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"gptq-4bit-128g-actorder_True\")\n",
    "\n",
    "# model = exllama_set_max_input_length(model, 2048)\n",
    "\n",
    "# model.save_pretrained(\"/home/ning/workdir/model/docsum-testmodel-v0.0.1\", from_pt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_wrd = \"Summary:\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-GPTQ\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sliding windows : evaluating prompts $\\geqslant$ 4096 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp/data/feature/test1.txt\") as f:\n",
    "    doc = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "result = nlp(doc)\n",
    "\n",
    "window_size = 1000\n",
    "grid_size = 2000\n",
    "shards_number = ceil(len(result)/grid_size)\n",
    "file_shards = [None] * shards_number\n",
    "\n",
    "for i in range(shards_number):\n",
    "\n",
    "    tmp = result[i *grid_size : i * grid_size + window_size]\n",
    "    file_shards[i] = ''.join([token.text_with_ws for token in tmp])\n",
    "\n",
    "    with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp/data/feature/test1_' + str(i) + '.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(file_shards[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    \n",
      "Hami Qutami was initially sentenced to 4 years and 5 months, with an 11 months and 21 days non-parole period for trying to hire someone to kill his niece Ruba due to cultural and religious disagreements. He challenged the decision, claiming that the sentence was far too lenient and the judge made mistakes when determining the non-parole period.\n",
      "\n",
      "The prosecution contended that reducing the non-parole period was unwarranted, despite acknowledging Qutami's good behavior and health. Nevertheless, they also maintained that the punishment was excessively lenient and failed to reflect the severity of the crime.\n",
      "\n",
      "Qutami, on the other hand, insisted that the sentence was fair and that the judge had not made any errors. He highlighted the importance of peace within communities and his medical issues.\n",
      "\n",
      "The court eventually found several problems with the initial ruling, declaring that the non-parole period was too brief and failed to adequately reflect the severity of the crime. Additionally, the judge's sympathy for Qutami resulted in an overly lenient view of the situation and an incorrectly shortened non-parole period.\n",
      "\n",
      "Section 44 of the 1999 Act requires courts to first establish the length of the sentence and subsequently determine the non-parole period for the offender. If the non-parole period is less than three-quarters of the term of the sentence, the court must identify unique circumstances and explain them in writing.\n",
      "\n",
      "In the case of R v Carrion, it was established that the focus should be on reducing the non-parole period rather than increasing the non-parole period. Furthermore, factors such as promoting healing within the family and community impacted by the event should be considered relevant when determining a shorter than usual non-parole period.\n",
      "\n",
      "Although the judge correctly took these aspects into account, the end outcome did not accurately represent the gravity of the crime. Therefore, while the judge's approach was legally valid, the sentence imposed was deemed too lenient and failed to properly reflect the seriousness of the offense.\n"
     ]
    }
   ],
   "source": [
    "short_sum = []\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "for i in range(shards_number):\n",
    "\n",
    "    with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp/data/feature/test1_' + str(i) + '.txt', 'r+', encoding=\"utf-8\") as file:\n",
    "        context = file.read()\n",
    "\n",
    "\n",
    "\n",
    "    prompt_template=f'''You are a helpful and patient AI assistant. You always provide all the details. Take a deep breathe and write a summary step by step about the following artile. Include all the details. \\n\n",
    "    Article: {short_sum} \\n {context} \\n \n",
    "    Summary: \n",
    "    '''\n",
    "\n",
    "    result = pipe(prompt_template)\n",
    "\n",
    "    raw_short_result = result[0]['generated_text']\n",
    "    short_sum_idx = raw_short_result.find(start_wrd)\n",
    "    short_sum = raw_short_result[short_sum_idx+8:]\n",
    "\n",
    "with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp/result/cumulated_sum.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(short_sum)\n",
    "\n",
    "print(short_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating prompts $\\leqslant$ 4096 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "This is a medical report for Mr Tan Ah Kow, a 55-year-old male patient. He has a history of various medical conditions including hypertension, hyperlipidemia, stroke, cardiomyopathy, cardiac failure, and chronic renal disease. He has been under the care of Dr. Tan Ah Moi since November 2010. During a recent examination on 20th June 2015, he displayed signs of cognitive impairment and was unable to recall important information or perform simple tasks. He was diagnosed with Dementia and stroke based on these observations. The doctor opines that Mr Tan's cognitive functions are unlikely to improve and may deteriorate further with time.\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp/data/feature/os-1.txt\") as f:\n",
    "    context = f.read()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "short_prompt_template=f'''You are a helpful and patient AI assistant. You always provide all the details. Take a deep breathe and write a long summary step by step about the following artile. Include all the details. \\n\n",
    " Article: {context} \\n \n",
    " Summary: \n",
    "'''\n",
    "\n",
    "result = pipe(short_prompt_template)\n",
    "\n",
    "raw_short_result = result[0]['generated_text']\n",
    "short_sum_idx = raw_short_result.find(start_wrd)\n",
    "short_sum = raw_short_result[short_sum_idx+8:]\n",
    "print(short_sum)\n",
    "\n",
    "with open('/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llamacpp/result/short_sum.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(short_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
